# testpy - Universal Test Orchestrator
# Multi-Language Support: Rust, Python, Node.js/npm, Shell Scripts
# Created: 2025-10-08

================================================================================
HIGH-LEVEL REQUIREMENTS
================================================================================

1. MULTI-LANGUAGE SUPPORT
   - Rust: cargo test integration, Cargo.toml detection
   - Python: pytest/unittest integration, pyproject.toml/setup.py detection
   - Node.js: npm/jest integration, package.json detection
   - Shell: Direct script execution, test discovery by convention

2. UNIFIED TEST ORGANIZATION
   - 9 test categories: sanity, smoke, unit, integration, e2e, uat, chaos, bench, regression
   - Naming pattern: <category>_<module>.<ext> (e.g., sanity_math.rs, uat_api.py, smoke_routes.test.js)
   - Required tests per module: sanity + UAT
   - Category entry points: <category>.<ext> (e.g., smoke.rs, unit.py, integration.test.js)

3. CONFIGURATION VIA .spec.toml
   - Unified config shared with feat-py and other RSB tooling
   - Language detection: languages = ["rust", "python", "nodejs", "shell"]
   - Test discovery patterns per language
   - Exclusion patterns per language
   - Project-specific overrides

4. TEST STRUCTURE VALIDATION
   - Naming convention enforcement
   - Required test detection (sanity + UAT per module)
   - Category entry file validation
   - Directory structure validation
   - Module discovery (language-agnostic)

5. TEST EXECUTION
   - Language-specific test runners (cargo, pytest, npm test, bash)
   - Timeout handling (prevent hangs)
   - Parallel execution support
   - Category-specific filtering
   - Module-specific filtering

6. REPORTING & OUTPUT
   - Boxy integration for pretty terminal output
   - Categorized violation reports
   - Override/bypass modes for emergencies
   - Plain output mode for CI/scripting (--view=data)
   - Test results summary

7. CROSS-PLATFORM SUPPORT
   - Linux, macOS, Windows compatibility
   - Path handling (pathlib)
   - Process spawning (subprocess with proper timeout)
   - Terminal detection (for boxy fallback)

8. INTEGRATION
   - feat-py integration for feature documentation
   - MODULE_SPEC awareness (per-language specs)
   - Brain registry potential (test coverage tracking)
   - CI/CD friendly (exit codes, JSON output)

================================================================================
MILESTONE ROADMAP
================================================================================

MILESTONE 1: Foundation & Infrastructure [WEEKS 1-2] âœ… COMPLETE
  Core architecture, configuration, repository detection
  Success: Can detect projects and load config across all target languages
  Status: 16/16 points (100%) - All 5 stories complete
  Date Completed: 2025-10-08

MILESTONE 2: Rust Support (Canonical Implementation) [WEEKS 3-4] âœ… COMPLETE
  Complete Rust testing support using RSB patterns
  Success: Can run and validate tests for RSB projects
  Status: 31/31 points (100%) - All 6 stories complete
  Date Completed: 2025-10-09 (including M2.6 hub validation)

MILESTONE 3: Multi-Language Discovery & Validation [WEEKS 5-6]
  Module discovery and test validation for Python/Node.js/Shell
  Success: Can discover modules and validate structure for all languages

MILESTONE 4: Documentation & Testing UX [WEEKS 7-8] ðŸ†•
  Rust testing documentation, ceremony support, visual UX guidance
  Success: Universal testing docs synced to brain, ceremony tests supported

MILESTONE 5: Test Execution Engine [WEEKS 9-10]
  Test runners for all languages with timeout and parallel support
  Success: Can execute tests across all language ecosystems

MILESTONE 6: Reporting & Polish [WEEKS 11-13]
  Beautiful reporting, boxy integration, CI/CD optimizations
  Success: Production-ready tool with comprehensive reporting

MILESTONE 7: Deployment & Documentation [WEEKS 14-15]
  Global deployment, migration guides, comprehensive docs
  Success: Tool deployed globally, 23 Rust projects aligned

================================================================================
MILESTONE 1: FOUNDATION & INFRASTRUCTURE âœ… COMPLETE
================================================================================

STORY: M1.1 - Core Project Structure [2 points] âœ… COMPLETE
  Commit: ded55fb
  Description: Set up Python package structure with proper organization

  Tasks:
    - Create src/testpy/ module structure
    - Set up __init__.py with version management
    - Create __main__.py for direct invocation
    - Update pyproject.toml with proper dependencies

  Requirements:
    - Python 3.8+ compatibility
    - Minimal dependencies (no heavy frameworks)
    - Proper package metadata

  Success Criteria:
    âœ“ Can import testpy as module
    âœ“ Can run via `python -m testpy`
    âœ“ Version shows correctly from pyproject.toml

  Hints:
    - Reuse feat-py version extraction pattern
    - Keep dependencies minimal (stdlib preferred)

---

STORY: M1.2 - Multi-Language Configuration System [5 points] âœ… COMPLETE
  Commit: 0225b20
  Description: Load and validate .spec.toml with multi-language support

  Tasks:
    - Create config.py with Config dataclass
    - Define language-specific configuration schemas
    - Implement fallback chains (spec â†’ manifest â†’ defaults)
    - Add validation for language-specific requirements
    - Support per-language test patterns and exclusions

  Requirements:
    - Unified .spec.toml format (shared with feat-py)
    - Per-language sections: [rust], [python], [nodejs], [shell]
    - Global settings: project_name, languages, test_root
    - Language-specific: test_patterns, exclusions, runner_options

  Success Criteria:
    âœ“ Can load .spec.toml with multi-language config
    âœ“ Falls back to Cargo.toml/pyproject.toml/package.json when needed
    âœ“ Validates language-specific requirements
    âœ“ Provides sensible defaults per language

  Hints:
    - Use tomllib (Python 3.11+) or tomli (fallback)
    - Reuse feat-py config patterns
    - Language detection: check manifest files
    - Default test patterns: tests/**/*_test.rs, tests/**/*_test.py, etc.

---

STORY: M1.3 - Repository Detection (Multi-Language) [3 points] âœ… COMPLETE
  Commit: c7c479a
  Description: Detect project type and validate requirements

  Tasks:
    - Create repo.py with RepoContext class
    - Detect language from manifest files
    - Validate language-specific requirements
    - Support multi-language projects (Rust+Python common)
    - Walk up directory tree to find repo root

  Requirements:
    - Must find .git directory (repo root)
    - Rust: requires Cargo.toml
    - Python: requires pyproject.toml, setup.py, or setup.cfg
    - Node.js: requires package.json
    - Shell: requires tests/ directory by convention
    - Multi-language: detect primary language by file count

  Success Criteria:
    âœ“ Detects all 4 language types correctly
    âœ“ Handles multi-language projects (Rust+Python bindings)
    âœ“ Provides clear error messages for missing requirements
    âœ“ Works from any subdirectory in repo

  Hints:
    - Reuse feat-py RepoContext pattern
    - Language detection: check for Cargo.toml, pyproject.toml, package.json
    - Primary language: count files by extension (.rs vs .py vs .js)
    - Support hybrid projects (e.g., PyO3 Rust+Python)

---

STORY: M1.4 - CLI Foundation [3 points] âœ… COMPLETE
  Commit: ca664a4
  Description: Create CLI interface with command routing

  Tasks:
    - Create cli.py with argparse setup
    - Define commands: run, lint, violations, check, docs
    - Add global options: --override, --skip-enforcement, --timeout
    - Implement --view=data for plain output (CI-friendly)
    - Set up command routing to handlers

  Requirements:
    - Commands: run [category] [module], lint, violations, check, docs
    - Global flags: --override, --skip-enforcement, --timeout=SECS, --parallel
    - Output modes: --view=data (plain), --view=pretty (boxy), --no-boxy
    - Language filtering: --language=rust|python|nodejs|shell
    - Exit codes: 0=success, 1=validation errors, 2=test failures

  Success Criteria:
    âœ“ All commands defined with proper help text
    âœ“ Global options work across commands
    âœ“ Proper exit codes for different error types
    âœ“ Help text is clear and comprehensive

  Hints:
    - Use argparse subcommands
    - Reuse boxy patterns from feat-py
    - Exit codes: 0=pass, 1=validation fail, 2=test fail, 127=config error

---

STORY: M1.5 - Boxy Integration & Output System [3 points] âœ… COMPLETE
  Commit: 34ab544
  Description: Unified output system with boxy pretty-printing

  Tasks:
    - Create output.py with boxy integration
    - Implement fallback for when boxy unavailable
    - Add themed output functions (success, warning, error, info)
    - Support --view=data plain output mode
    - Handle REPOS_USE_BOXY environment variable

  Requirements:
    - Check boxy availability (shutil.which)
    - Fallback to plain stderr output
    - Themes: magic, success, warning, error, plain
    - Plain mode for CI (--view=data or REPOS_USE_BOXY=1)
    - Width control (--width=max|80|100)

  Success Criteria:
    âœ“ Boxy displays pretty output when available
    âœ“ Gracefully falls back to plain output
    âœ“ --view=data disables boxy completely
    âœ“ All themes work correctly

  Hints:
    - Copy boxy_display pattern from feat-py
    - subprocess.run with capture_output for boxy calls
    - Fallback: print to stderr with [theme] prefix

---

MILESTONE 1 TOTAL: 16 story points

================================================================================
MILESTONE 2: RUST SUPPORT (CANONICAL IMPLEMENTATION) âœ… COMPLETE
Status: 31/31 points (100%) - All 6 stories complete
Date Completed: 2025-10-09 (including M2.6 hub validation)
================================================================================

STORY: M2.1 - Rust Module Discovery [5 points] âœ… COMPLETE
  Commit: f1f7fc4
  Description: Discover Rust modules from src/ directory

  Tasks:
    - Create discovery.py with Rust-specific discovery
    - Pattern 1: src/module.rs files (legacy, optional)
    - Pattern 2: src/module/mod.rs files (MODULE_SPEC standard)
    - Apply exclusion patterns (_*, dev_*, prelude*, lib, main)
    - Detect module public API via pub keyword scan

  Requirements:
    - Scan src/ for module/mod.rs directories
    - Support legacy src/module.rs files (with opt-in)
    - Exclusions: _*, dev_*, prelude*, dummy_*, lib, main, macros, streamable, xcls
    - Return module names with file paths
    - Handle nested modules (src/pkg/module/mod.rs)

  Success Criteria:
    âœ“ Discovers all non-excluded modules in RSB project
    âœ“ Handles both mod.rs and .rs patterns
    âœ“ Correctly applies exclusion patterns
    âœ“ Returns structured module data

  Hints:
    - Use pathlib.Path.glob for discovery
    - Reuse exclusion logic from rsb test.sh
    - Special case: 'dev' module is NOT excluded, only dev_*

---

STORY: M2.2 - Rust Test Discovery [5 points] âœ… COMPLETE
  Commit: f1f7fc4 (implemented with M2.1)
  Description: Discover Rust test files in tests/ directory

  Tasks:
    - Scan tests/*.rs for wrapper files
    - Scan tests/<category>/*.rs for organized tests
    - Parse test file naming: <category>_<module>.rs
    - Map tests to modules
    - Detect category entry files (sanity.rs, unit.rs, etc.)

  Requirements:
    - Valid categories: sanity, smoke, unit, integration, e2e, uat, chaos, bench, regression
    - Naming: <category>_<module>.rs or <category>/<module>.rs
    - Skip: _*, dev_* patterns
    - Detect: category entry files (smoke.rs, unit.rs)
    - Return: test name, category, module, file path

  Success Criteria:
    âœ“ Discovers all test files in RSB test suite
    âœ“ Correctly parses category and module from filename
    âœ“ Maps tests to discovered modules
    âœ“ Identifies category entry files

  Hints:
    - Regex for parsing: ^(sanity|smoke|...)_([a-z_]+)\.rs$
    - Support both wrapper style and directory style
    - Category entries are just category name (no underscore)

---

STORY: M2.3 - Rust Test Validation [8 points] âœ… COMPLETE
  Commit: e500965
  Description: Validate Rust test organization against RSB rules

  Tasks:
    - Create validator.py with validation logic
    - Check naming conventions (<category>_<module>.rs)
    - Detect missing sanity tests per module
    - Detect missing UAT tests per module
    - Validate category entry files exist
    - Check for unauthorized root test files
    - Detect invalid test directories
    - Categorize violations by type

  Requirements:
    - 6 violation types:
      1. Naming violations (wrong pattern)
      2. Missing sanity tests (no sanity_<module>.rs)
      3. Missing UAT tests (no uat_<module>.rs)
      4. Missing category entries (no smoke.rs, etc.)
      5. Unauthorized root files (tests/foo.rs not matching pattern)
      6. Invalid directories (tests/foo/ not a valid category)
    - Support 3 test file locations:
      - tests/<category>_<module>.rs (wrapper)
      - tests/<category>/<module>.rs (directory)
      - tests/<category>/<category>_<module>.rs (prefixed)

  Success Criteria:
    âœ“ Detects all 6 violation types correctly
    âœ“ Supports all 3 test file location patterns
    âœ“ Returns categorized violations
    âœ“ Matches rsb test.sh validation logic exactly

  Hints:
    - Reference: rsb_test.sh validate_test_structure()
    - Use discovery results to cross-reference modules
    - Required: sanity + UAT for each discovered module
    - Category entries: sanity.rs, smoke.rs, unit.rs, etc.

---

STORY: M2.4 - Rust Test Runner [5 points] âœ… COMPLETE
  Commit: b1801b2
  Description: Execute Rust tests via cargo with timeout

  Tasks:
    - Create runner.py with cargo test execution
    - Implement timeout wrapper (like ctest in bash)
    - Support category filtering (cargo test --test sanity_*)
    - Support module filtering (cargo test --test *_math)
    - Capture and parse cargo test output

  Requirements:
    - Run: cargo test with subprocess
    - Timeout: configurable (default 600s), use timeout/gtimeout commands
    - Filtering: --test flag for category/module
    - Output: capture stdout/stderr
    - Exit code: pass through cargo exit code

  Success Criteria:
    âœ“ Can run cargo test with timeout
    âœ“ Can filter by category (all sanity tests)
    âœ“ Can filter by module (all math tests)
    âœ“ Timeout kills hung tests properly
    âœ“ Returns structured test results

  Hints:
    - subprocess.run with timeout parameter
    - Fallback: use 'timeout' command if available
    - cargo test --test <pattern> for filtering
    - Parse output for test counts (passed, failed, ignored)

---

STORY: M2.5 - Rust Violation Reporting [5 points] âœ… COMPLETE
  Commit: b1801b2
  Description: Generate beautiful violation reports for Rust

  Tasks:
    - Create formatted violation reports by category
    - Show file-level violations with line numbers
    - Show module-level violations with suggestions
    - Generate fix recommendations
    - Display via boxy with warning theme

  Requirements:
    - Group by violation type
    - Show counts per type
    - List specific violations with context
    - Provide actionable fix suggestions
    - Display summary with quick fixes
    - Support both pretty (boxy) and plain (--view=data) output

  Success Criteria:
    âœ“ Matches rsb test.sh violation report format
    âœ“ Clear, actionable recommendations
    âœ“ Beautiful boxy display with warning theme
    âœ“ Plain text fallback for CI

  Hints:
    - Reference: rsb_test.sh lines 283-399 (violation report)
    - Use boxy_display with "warning" theme
    - Format: section header, list items, fix suggestions
    - Summary box at end with counts and quick fixes

---

STORY: M2.6 - Hub Integration Test Validation [3 points] âœ… COMPLETE
  Commits: 6df0875, baa93ec, c39c272
  Date Completed: 2025-10-09
  Description: Enforce integration tests for hub packages

  Tasks:
    âœ“ Implement get_hub_packages() function
      - Parse blade cache: ~/.local/data/snek/blade/deps_cache.tsv
      - Extract hub repo dependencies (REPO_ID with name="hub")
      - Return list of package names

    âœ“ Implement has_hub_usage() function
      - Check if repo uses hub (src/deps.rs exists)
      - Return boolean

    âœ“ Add validate_hub_integration_tests() to validator
      - For each hub package, check for tests/integration/hub_<package>.rs
      - Return list of missing hub integration tests
      - Add as 7th violation type: "missing_hub_integration"

    âœ“ Update violation reporting
      - Display missing hub integration tests
      - Format: "Hub package '<package>' missing integration test"
      - Expected: "tests/integration/hub_<package>.rs"

    âœ“ Update RUST_TEST_CHECKLIST.md
      - Document hub integration test requirement
      - Explain pattern: hub_<package>.rs
      - Show example lightweight sanity test

  Requirements:
    - Only validate if repo uses hub (src/deps.rs exists)
    - Parse blade cache (fallback: skip if cache missing)
    - Lightweight sanity tests (import + basic usage check)
    - Non-blocking validation (warning, not error)

  Acceptance Criteria:
    - Detects hub usage via src/deps.rs
    - Reads hub packages from blade cache
    - Validates tests/integration/hub_*.rs exist
    - Reports missing hub integration tests
    - Documentation updated

  Example Hub Integration Test:
    ```rust
    // tests/integration/hub_chrono.rs
    #[test]
    fn hub_chrono_available() {
        use rsb::deps::chrono::Utc;
        let now = Utc::now();
        assert!(now.timestamp() > 0);
    }
    ```

  Story Points: 3 (simple validation logic + docs)

---

MILESTONE 2 TOTAL: 31 story points (was 28 + 3 for M2.6)

================================================================================
MILESTONE 3: MULTI-LANGUAGE DISCOVERY & VALIDATION
================================================================================

STORY: M3.1 - Python Module Discovery [3 points]
  Description: Discover Python modules from src/ or project root

  Tasks:
    - Scan for Python packages (dirs with __init__.py)
    - Scan for standalone modules (*.py files)
    - Apply Python-specific exclusions (__pycache__, *.pyc, _*, dev_*)
    - Detect module public API via class/function scan

  Requirements:
    - Package pattern: src/<package>/__init__.py or <package>/__init__.py
    - Module pattern: src/<module>.py or <module>.py
    - Exclusions: __pycache__, *.pyc, _*, dev_*, test_*, conftest
    - Return module names with paths

  Success Criteria:
    âœ“ Discovers Python packages and modules
    âœ“ Handles both src/ layout and flat layout
    âœ“ Correctly excludes test files and caches
    âœ“ Returns structured module data

  Hints:
    - Check for __init__.py to identify packages
    - Standalone modules: *.py in src/ or root (not tests/)
    - Common pattern: src/<package>/ with __init__.py

---

STORY: M3.2 - Python Test Discovery [3 points]
  Description: Discover Python tests (pytest/unittest patterns)

  Tasks:
    - Scan tests/ for test_*.py and *_test.py
    - Parse category from filename: <category>_<module>_test.py
    - Support pytest discovery patterns
    - Map tests to modules
    - Detect category entry files

  Requirements:
    - Patterns: test_*.py, *_test.py, <category>_<module>_test.py
    - Categories: same 9 as Rust (sanity, smoke, unit, ...)
    - Directory structure: tests/<category>/*.py
    - Return: test name, category, module, file path

  Success Criteria:
    âœ“ Discovers pytest-style test files
    âœ“ Parses category and module from filenames
    âœ“ Maps tests to Python modules
    âœ“ Handles pytest conventions

  Hints:
    - pytest patterns: test_*.py or *_test.py
    - Category parsing: sanity_api_test.py â†’ category=sanity, module=api
    - Support tests/<category>/test_<module>.py structure

---

STORY: M3.3 - Python Test Validation [5 points]
  Description: Validate Python test organization

  Tasks:
    - Adapt Rust validation logic for Python
    - Check Python test naming conventions
    - Detect missing sanity/UAT tests per module
    - Validate category entry files
    - Python-specific: check for conftest.py

  Requirements:
    - Naming: <category>_<module>_test.py or test_<category>_<module>.py
    - Required: sanity + UAT per module
    - Category entries: test_sanity.py, test_smoke.py, etc.
    - Validate conftest.py presence for fixtures

  Success Criteria:
    âœ“ Validates Python test organization
    âœ“ Detects missing required tests
    âœ“ Checks naming conventions
    âœ“ Returns categorized violations

  Hints:
    - Reuse validator.py, add PythonValidator class
    - Adapt violation types for Python conventions
    - pytest expects test_* or *_test.py

---

STORY: M3.4 - Node.js Module Discovery [3 points]
  Description: Discover Node.js modules from src/ or lib/

  Tasks:
    - Scan for JavaScript/TypeScript modules
    - Support both CommonJS and ES modules
    - Apply Node.js exclusions (node_modules, dist, build)
    - Detect module exports

  Requirements:
    - Patterns: src/**/*.js, src/**/*.ts, lib/**/*.js
    - Module detection: module.exports or export keyword
    - Exclusions: node_modules, dist, build, _*, dev_*
    - Return module names with paths

  Success Criteria:
    âœ“ Discovers JavaScript/TypeScript modules
    âœ“ Handles CommonJS and ES module syntax
    âœ“ Excludes build artifacts and node_modules
    âœ“ Returns structured module data

  Hints:
    - Check package.json for "type": "module" (ES modules)
    - Scan for export/module.exports to identify modules
    - Common layouts: src/, lib/, index.js

---

STORY: M3.5 - Node.js Test Discovery [3 points]
  Description: Discover Node.js tests (jest/mocha patterns)

  Tasks:
    - Scan for *.test.js, *.spec.js patterns
    - Parse category from filename
    - Support test/ and __tests__/ directories
    - Map tests to modules

  Requirements:
    - Patterns: *.test.js, *.spec.js, <category>.<module>.test.js
    - Categories: same 9 categories
    - Directories: test/, tests/, __tests__/
    - Return: test name, category, module, file path

  Success Criteria:
    âœ“ Discovers jest/mocha style tests
    âœ“ Parses category and module from filenames
    âœ“ Maps tests to Node.js modules
    âœ“ Handles both test/ and __tests__/ conventions

  Hints:
    - jest patterns: *.test.js, *.spec.js
    - Category parsing: sanity.api.test.js â†’ category=sanity, module=api
    - Support __tests__/<module>/<category>.test.js

---

STORY: M3.6 - Node.js Test Validation [5 points]
  Description: Validate Node.js test organization

  Tasks:
    - Adapt validation logic for Node.js
    - Check Node.js test naming conventions
    - Detect missing sanity/UAT tests per module
    - Validate jest/mocha configuration

  Requirements:
    - Naming: <category>.<module>.test.js or <module>.<category>.spec.js
    - Required: sanity + UAT per module
    - Category entries: sanity.test.js, smoke.test.js
    - Validate test framework config (jest.config.js, .mocharc.js)

  Success Criteria:
    âœ“ Validates Node.js test organization
    âœ“ Detects missing required tests
    âœ“ Checks naming conventions
    âœ“ Returns categorized violations

  Hints:
    - Reuse validator.py, add NodeValidator class
    - jest/mocha have different conventions
    - Check package.json for test framework

---

STORY: M3.7 - Shell Script Discovery [3 points]
  Description: Discover shell scripts and test scripts

  Tasks:
    - Scan for *.sh scripts in src/, bin/, scripts/
    - Scan for test scripts in tests/
    - Parse category from test script names
    - Validate shebang and executable bit

  Requirements:
    - Script patterns: src/**/*.sh, bin/**/*.sh, scripts/**/*.sh
    - Test patterns: tests/<category>_<module>.sh
    - Validate shebang: #!/bin/bash or #!/bin/sh
    - Check executable bit (stat)

  Success Criteria:
    âœ“ Discovers shell scripts by location
    âœ“ Identifies executable test scripts
    âœ“ Parses category from test names
    âœ“ Validates shebang presence

  Hints:
    - Use pathlib for executable check: path.stat().st_mode
    - Shebang: read first line, check for #!
    - Common locations: bin/, scripts/, tests/sh/

---

STORY: M3.8 - Shell Script Validation [3 points]
  Description: Validate shell test organization

  Tasks:
    - Check shell test naming conventions
    - Detect missing sanity/UAT test scripts
    - Validate executable permissions
    - Check for proper shebangs

  Requirements:
    - Naming: <category>_<module>.sh
    - Required: sanity + UAT per script module
    - Executable: chmod +x
    - Shebang: #!/bin/bash (preferred) or #!/bin/sh

  Success Criteria:
    âœ“ Validates shell test organization
    âœ“ Checks executable permissions
    âœ“ Validates shebangs
    âœ“ Returns categorized violations

  Hints:
    - Check os.access(path, os.X_OK) for executable
    - Shebang check: open file, read first line
    - Shell tests often in tests/sh/ subdirectory

---

MILESTONE 3 TOTAL: 28 story points

================================================================================
MILESTONE 4: DOCUMENTATION & TESTING UX ðŸ†•
================================================================================

Reference: TEST_UX_UPDATE.md (comprehensive planning document)

STORY: M4.1 - Create Rust Test Organization Documentation [5 points]
  Description: Adapt RSB TEST_ORGANIZATION.md for universal Rust testing

  Tasks:
  â–¡ Adapt TEST_ORGANIZATION.md â†’ docs/rust/RUST_TEST_ORGANIZATION.md
    - Remove RSB-specific references
    - Replace test.sh commands with testpy commands
    - Generalize for any Rust project using RSB patterns
    - Keep all structure/pattern definitions
    - Add ceremony tests section (optional)

  Acceptance Criteria:
  - Universal Rust test organization standard (363 lines adapted)
  - Test categories documented (9 standard + ceremony optional)
  - Naming patterns explained
  - Required tests per module (sanity + UAT)
  - 6 violation types documented
  - Ceremony test structure explained

  Story Points: 5 (medium complexity, requires RSBâ†’generic adaptation)


STORY: M4.2 - Create Rust Testing HOWTO Guide [5 points]
  Description: Adapt RSB HOWTO_TEST.md with testpy commands

  Tasks:
  â–¡ Adapt HOWTO_TEST.md â†’ docs/rust/RUST_TESTING_HOWTO.md
    - Replace ./bin/test.sh with testpy commands
    - Update command examples throughout
    - Keep examples and best practices
    - Add ceremony command section
    - Update Quick Start guide

  Acceptance Criteria:
  - Complete testing guide (766 lines adapted)
  - All testpy commands documented
  - Module-based testing explained
  - Category explanations included
  - Test writing examples provided
  - Ceremony tests documented

  Story Points: 5 (medium complexity, command replacement + content updates)


STORY: M4.3 - Create Visual Testing UX Documentation [3 points]
  Description: Extract visual UX principles from architecture docs

  Tasks:
  â–¡ Create docs/rust/RUST_TESTING_UX.md from:
    - BASHFX-v3.md section 4.5 (Visual Friendliness, lines 790-820)
    - RSB_ARCH.md section 4.2 (Test Runner and Ceremony, lines 623-667)
    - HOWTO_UPDATE_RSB.md Chapter 6.2 (Test Ceremony Commands)

  Content Sections:
  â–¡ Visual Friendliness Principles
  â–¡ UAT Visual Requirements (hypothesis â†’ expectation â†’ result)
  â–¡ Test Output Structure (numbered, titled, STATUS messages)
  â–¡ Glyphs and Colors (âœ“, âœ—, âš , â„¹)
  â–¡ Formatting Techniques (box drawing, indentation, spacing)
  â–¡ Summary Ceremonies (metrics, timing, failed tests)
  â–¡ Tool Integration (reference to RUST_BOXY_ROLO_USAGE.md)
  â–¡ Examples (simple UAT, enhanced UAT, ceremony)

  Acceptance Criteria:
  - Visual UX principles documented (NOT ceremony-specific)
  - UAT requirements clearly explained
  - Simple println formatting is sufficient
  - Cross-reference to boxy/rolo doc
  - Example UAT test patterns provided

  Story Points: 3 (lower complexity, extraction + organization)


STORY: M4.4 - Create Boxy/Rolo Usage Documentation [3 points]
  Description: Document boxy and rolo integration for enhanced testing

  Tasks:
  â–¡ Create docs/rust/RUST_BOXY_ROLO_USAGE.md with:
    - Boxy v0.23.0 usage patterns
    - Rolo v0.2 usage patterns (NOTE: docs being cleaned up)
    - Combined ceremony patterns
    - Real examples from boxy/tests/ceremonies/

  Content Sections:
  â–¡ Tool Overview (versions, API reference commands)
  â–¡ When to Use These Tools (ceremonies, not required for UAT)
  â–¡ Boxy Basics (themes, title/status/footer, layout)
  â–¡ Boxy Advanced (param streams, progressive disclosure)
  â–¡ Rolo Basics (table/column mode, with cleanup note)
  â–¡ Combined Patterns (table in box, multi-step ceremony)
  â–¡ Ceremony Script Templates
  â–¡ Real Examples (from boxy upstream)
  â–¡ Cross-References (to RUST_TESTING_UX.md)

  Acceptance Criteria:
  - Boxy usage documented with examples
  - Rolo usage documented with cleanup warning
  - Combined patterns demonstrated
  - Ceremony templates provided
  - Cross-references included

  Story Points: 3 (lower complexity, tool documentation)


STORY: M4.5 - Implement docs Command [5 points]
  Description: Add CLI command to display testing documentation

  Tasks:
  â–¡ Add docs subcommand to cli.py
  â–¡ Implement doc path resolution (docs/rust/*.md)
  â–¡ Add boxy-themed doc display with fallback
  â–¡ Support doc aliases:
    - rust-lint (or rust-org) â†’ RUST_TEST_ORGANIZATION.md
    - rust-howto â†’ RUST_TESTING_HOWTO.md
    - rust-ux â†’ RUST_TESTING_UX.md
    - rust-boxy (or rust-tools) â†’ RUST_BOXY_ROLO_USAGE.md

  Implementation:
  â–¡ cmd_docs(args, config, repo) function
  â–¡ Doc resolver with pattern matching
  â–¡ Boxy display with title formatting
  â–¡ Plain output mode (--view=data)
  â–¡ Error handling for missing docs

  Acceptance Criteria:
  - testpy docs rust-lint displays RUST_TEST_ORGANIZATION.md
  - testpy docs rust-howto displays RUST_TESTING_HOWTO.md
  - testpy docs rust-ux displays RUST_TESTING_UX.md
  - testpy docs rust-boxy displays RUST_BOXY_ROLO_USAGE.md
  - Boxy theming with graceful fallback
  - Plain mode works (--view=data)

  Story Points: 5 (medium complexity, CLI + display logic)


STORY: M4.6 - Implement sync Command [5 points]
  Description: Sync universal testing docs to brain registry

  Tasks:
  â–¡ Implement sync_to_brain() function
  â–¡ Target directory: ~/repos/docs/brain/dev/procs/testing/rust/
  â–¡ Copy all docs/rust/*.md files
  â–¡ Create target directories if needed
  â–¡ Report sync status (files copied)

  Implementation:
  â–¡ cmd_sync(args, config, repo) function
  â–¡ Brain path detection/creation
  â–¡ File copying with shutil.copy2
  â–¡ Summary output with boxy formatting

  Acceptance Criteria:
  - testpy sync copies docs/rust/*.md to brain
  - Target: ~/repos/docs/brain/dev/procs/testing/rust/
  - Creates directories if needed
  - Reports files synced
  - Graceful error handling

  Story Points: 5 (medium complexity, file operations + path handling)


STORY: M4.7 - Implement Ceremony Support [8 points]
  Description: Add ceremony test discovery and execution

  Tasks:
  â–¡ Implement ceremony discovery:
    - Find tests/ceremonies/ceremony_*.{sh,rs,py}
    - Extract <name> from ceremony_<name>.*
    - Return flat list of ceremony names

  â–¡ Add ceremony CLI commands:
    - testpy ceremony (alias for list)
    - testpy ceremony list
    - testpy ceremony run <name>
    - testpy ceremony run all

  â–¡ Implement ceremony execution:
    - Shell scripts: subprocess with bash
    - Rust files: cargo test with filter
    - Python files: pytest with filter
    - Capture output and report results

  â–¡ Update validator:
    - Ignore tests/ceremonies/ directory
    - No enforcement for ceremonies
    - Optional pattern validation if present

  Implementation:
  â–¡ discover_ceremonies() in discovery.py
  â–¡ cmd_ceremony() in cli.py
  â–¡ execute_ceremony() in runner.py (new function)
  â–¡ Update validator to ignore ceremonies/

  Acceptance Criteria:
  - testpy ceremony list shows all ceremonies
  - testpy ceremony run <name> executes ceremony
  - Shell scripts execute via bash
  - Rust/Python ceremonies execute via test runners
  - Validator ignores ceremonies directory
  - No enforcement (optional tests)

  Story Points: 8 (higher complexity, multi-language execution)


STORY: M4.8 - Testing and Documentation [5 points]
  Description: Test M4 features and update project docs

  Tasks:
  â–¡ Test docs command on RSB project
  â–¡ Test sync command (verify brain sync)
  â–¡ Test ceremony discovery and execution
  â–¡ Update testpy README.md:
    - Add docs command section
    - Add ceremony command section
    - Add brain sync information
  â–¡ Create usage examples in README
  â–¡ Update CONTINUE.md with M4 status

  Acceptance Criteria:
  - All M4 features tested on RSB
  - README updated with new commands
  - CONTINUE.md reflects M4 completion
  - Usage examples provided

  Story Points: 5 (testing + documentation)


MILESTONE 4 TOTAL: 44 story points

================================================================================
MILESTONE 5: TEST EXECUTION ENGINE
================================================================================

STORY: M4.1 - Python Test Runner [3 points]
  Description: Execute Python tests via pytest/unittest

  Tasks:
    - Detect test framework (pytest vs unittest)
    - Run pytest with proper arguments
    - Support category/module filtering
    - Capture and parse output
    - Implement timeout

  Requirements:
    - Auto-detect: pytest if available, else unittest
    - Timeout: configurable (default 600s)
    - Filtering: -k flag for pytest, pattern for unittest
    - Output: capture and parse results
    - Exit code: pass through test framework exit code

  Success Criteria:
    âœ“ Runs pytest tests correctly
    âœ“ Falls back to unittest if needed
    âœ“ Filters by category and module
    âœ“ Timeout works properly
    âœ“ Returns structured results

  Hints:
    - Check for pytest: shutil.which("pytest")
    - pytest command: pytest tests/<category>/ -v
    - unittest: python -m unittest discover
    - Parse pytest output for test counts

---

STORY: M4.2 - Node.js Test Runner [3 points]
  Description: Execute Node.js tests via npm/jest/mocha

  Tasks:
    - Detect test framework from package.json
    - Run npm test or framework directly
    - Support category/module filtering
    - Capture and parse output
    - Implement timeout

  Requirements:
    - Detect: check package.json "scripts": {"test": ...}
    - Frameworks: jest, mocha, tap, ava
    - Timeout: configurable
    - Filtering: framework-specific (jest -t, mocha --grep)
    - Exit code: pass through

  Success Criteria:
    âœ“ Runs npm test correctly
    âœ“ Supports jest and mocha filtering
    âœ“ Timeout works properly
    âœ“ Returns structured results

  Hints:
    - Check package.json for test script
    - jest filtering: jest --testNamePattern=<category>
    - mocha filtering: mocha --grep <pattern>
    - Parse framework-specific output

---

STORY: M4.3 - Shell Test Runner [2 points]
  Description: Execute shell test scripts directly

  Tasks:
    - Run shell scripts with subprocess
    - Capture stdout/stderr
    - Implement timeout
    - Support category/module filtering

  Requirements:
    - Execute: bash <script> or ./<script>
    - Timeout: configurable
    - Filtering: by filename pattern
    - Exit code: 0=pass, non-zero=fail

  Success Criteria:
    âœ“ Runs shell test scripts
    âœ“ Captures output correctly
    âœ“ Timeout works
    âœ“ Exit codes propagate correctly

  Hints:
    - Use subprocess.run with shell=False
    - Pass script path directly: ["/bin/bash", script_path]
    - Check executable bit before running
    - Timeout via timeout parameter

---

STORY: M4.4 - Parallel Test Execution [5 points]
  Description: Run tests in parallel across categories/modules

  Tasks:
    - Implement parallel test runner
    - Use multiprocessing or concurrent.futures
    - Support --parallel flag with worker count
    - Aggregate results from parallel runs
    - Handle failures and timeouts gracefully

  Requirements:
    - Parallel modes: by category, by module, by file
    - Worker pool: configurable size (default: CPU count)
    - Collect results: aggregate pass/fail counts
    - Timeout: per-test timeout, not global
    - Graceful shutdown on Ctrl+C

  Success Criteria:
    âœ“ Runs tests in parallel correctly
    âœ“ Aggregates results properly
    âœ“ Handles timeouts per-test
    âœ“ Faster than sequential for large suites
    âœ“ Ctrl+C stops cleanly

  Hints:
    - Use concurrent.futures.ProcessPoolExecutor
    - Each worker runs one category or module
    - Aggregate results via futures.as_completed()
    - Timeout: per-worker timeout, not pool timeout

---

STORY: M4.5 - Test Result Parsing [5 points]
  Description: Parse test output from all frameworks

  Tasks:
    - Parse cargo test output (Rust)
    - Parse pytest output (Python)
    - Parse jest/mocha output (Node.js)
    - Parse shell script exit codes
    - Normalize results to common format

  Requirements:
    - Extract: test count, passed, failed, ignored, time
    - Handle: various output formats (TAP, JUnit, custom)
    - Normalize: TestResult dataclass with standard fields
    - Errors: capture and categorize (compile, runtime, timeout)

  Success Criteria:
    âœ“ Parses all 4 language test outputs
    âœ“ Normalizes to common TestResult format
    âœ“ Extracts counts and timing info
    âœ“ Categorizes errors properly

  Hints:
    - cargo test: look for "test result: ok. 10 passed; 0 failed"
    - pytest: look for "== 10 passed in 1.23s =="
    - jest: look for "Tests: 10 passed, 10 total"
    - Regex patterns for each framework

---

STORY: M4.6 - Timeout & Hang Prevention [3 points]
  Description: Robust timeout handling to prevent test hangs

  Tasks:
    - Implement per-test timeout
    - Detect hung processes (no output for N seconds)
    - Kill timeout processes cleanly
    - Report timeout violations

  Requirements:
    - Timeout: configurable per-test (default 600s)
    - Hung detection: no output for 60s (configurable)
    - Kill: SIGTERM first, SIGKILL after grace period
    - Report: which test timed out, how long it ran

  Success Criteria:
    âœ“ Kills tests that exceed timeout
    âœ“ Reports timeout violations clearly
    âœ“ Doesn't leave zombie processes
    âœ“ Works across all language runners

  Hints:
    - subprocess.run with timeout parameter
    - Catch TimeoutExpired exception
    - Use psutil for reliable process termination
    - Grace period: 5s between SIGTERM and SIGKILL

---

MILESTONE 4 TOTAL: 21 story points

================================================================================
MILESTONE 5: REPORTING & POLISH
================================================================================

STORY: M5.1 - Test Results Dashboard [5 points]
  Description: Beautiful test results summary display

  Tasks:
    - Create results dashboard with boxy
    - Show per-category results
    - Show per-module results
    - Show timing information
    - Highlight failures prominently

  Requirements:
    - Display: category breakdown, module breakdown, timing
    - Colors: green=pass, red=fail, yellow=ignored, gray=skipped
    - Boxy themes: success (all pass), warning (some fail), error (all fail)
    - Summary: total tests, passed, failed, ignored, time

  Success Criteria:
    âœ“ Beautiful boxy display of results
    âœ“ Clear pass/fail indicators
    âœ“ Shows timing info
    âœ“ Plain fallback for CI

  Hints:
    - Use boxy with "success", "warning", or "error" theme
    - Format: table with category/module rows, pass/fail columns
    - Summary box at end with totals

---

STORY: M5.2 - Violation Report Enhancement [3 points]
  Description: Enhanced violation reports with fix suggestions

  Tasks:
    - Add per-language violation reports
    - Show code snippets for violations
    - Generate auto-fix commands
    - Link to documentation for fixes

  Requirements:
    - Show: file path, line number, violation type, suggestion
    - Auto-fix: generate commands to fix naming, create stubs
    - Docs: link to MODULE_SPEC, naming conventions
    - Support: all 4 languages

  Success Criteria:
    âœ“ Clear violation reports per language
    âœ“ Actionable fix suggestions
    âœ“ Auto-fix commands when possible
    âœ“ Links to relevant docs

  Hints:
    - Format: file:line - violation - suggestion
    - Auto-fix: "mv tests/foo.rs tests/sanity_foo.rs"
    - Docs links: "See: docs/ref/rsb/MODULE_SPEC.md"

---

STORY: M5.3 - CI/CD Optimizations [3 points]
  Description: Optimize for CI/CD pipeline usage

  Tasks:
    - Add --view=data for plain output (no boxy)
    - Add --quiet mode for minimal output
    - Add --json output format
    - Optimize exit codes for CI
    - Add --fail-fast mode

  Requirements:
    - --view=data: plain text, no colors, no boxy
    - --quiet: only show failures and summary
    - --json: structured JSON output for parsing
    - Exit codes: 0=pass, 1=validation fail, 2=test fail, 127=error
    - --fail-fast: stop on first failure

  Success Criteria:
    âœ“ Plain output works in CI
    âœ“ JSON output is parseable
    âœ“ Exit codes are reliable
    âœ“ --fail-fast stops quickly

  Hints:
    - --view=data: disable all boxy, no ANSI colors
    - --json: use json.dumps for results
    - Exit codes: return early on first failure for --fail-fast
    - CI detection: check for CI env var

---

STORY: M5.4 - Documentation Command [3 points]
  Description: Integrate docs command like rsb test.sh

  Tasks:
    - Add 'docs' command to CLI
    - Display MODULE_SPEC from docs/ref/rsb/
    - Display feature docs from docs/feats/
    - Integrate with feat-py brain registry
    - Support --view=data for plain output

  Requirements:
    - Command: testpy docs [feature]
    - Display: MODULE_SPEC.md if no arg, feature doc if specified
    - Boxy: pretty display with "info" theme
    - Brain: fall back to brain registry if local not found

  Success Criteria:
    âœ“ Can display MODULE_SPEC
    âœ“ Can display feature docs
    âœ“ Integrates with feat-py brain
    âœ“ Pretty boxy display

  Hints:
    - Reuse feat-py docs command pattern
    - Check local first: docs/ref/rsb/MODULE_SPEC.md
    - Fall back to brain: ~/repos/docs/brain/dev/proj/feat/<project>/
    - Use boxy "info" theme

---

STORY: M5.5 - Override & Bypass Modes [2 points]
  Description: Emergency bypass for validation enforcement

  Tasks:
    - Implement --override flag
    - Show warning when override active
    - Skip validation in override mode
    - Log override usage for audit

  Requirements:
    - --override: skip all validation, run tests anyway
    - Warning: big red warning via boxy when override active
    - Log: record when override used (for audit)
    - Still report violations, but don't block

  Success Criteria:
    âœ“ Override bypasses validation
    âœ“ Clear warning displayed
    âœ“ Violations still reported
    âœ“ Tests run despite violations

  Hints:
    - Check --override flag in validator
    - Show warning via boxy "warning" theme
    - Log to stderr: "WARNING: Running in override mode"
    - Still call validate, but ignore result

---

STORY: M5.6 - Error Handling & User Messages [3 points]
  Description: Comprehensive error handling with helpful messages

  Tasks:
    - Add error handling for all failure modes
    - Provide helpful error messages
    - Suggest fixes for common errors
    - Handle edge cases gracefully

  Requirements:
    - Errors: config not found, invalid language, no tests found, etc.
    - Messages: clear, actionable, not technical jargon
    - Suggestions: "Did you mean...?", "Try running..."
    - Exit codes: appropriate for error type

  Success Criteria:
    âœ“ All error paths have clear messages
    âœ“ Suggestions are helpful
    âœ“ No stack traces for user errors
    âœ“ Exit codes match error type

  Hints:
    - Try/except with specific exceptions
    - Error messages: avoid "Error: ", use friendly language
    - Suggestions: "Run 'testpy init' to create .spec.toml"
    - Stack traces: only in debug mode (--debug flag)

---

MILESTONE 5 TOTAL: 19 story points

================================================================================
MILESTONE 6: DEPLOYMENT & DOCUMENTATION
================================================================================

STORY: M6.1 - Deployment Script [3 points]
  Description: Global deployment via bin/deploy.sh

  Tasks:
    - Create bin/deploy.sh (similar to feat-py)
    - Install to ~/.local/bin/snek/testpy
    - Inject version from pyproject.toml
    - Create symlinks if needed
    - Add to PATH instructions

  Requirements:
    - Install location: ~/.local/bin/snek/testpy
    - Version injection: read from pyproject.toml, inject into __version__
    - Symlink: optional testpy symlink in ~/.local/bin/
    - PATH: check if ~/.local/bin in PATH, warn if not

  Success Criteria:
    âœ“ Deploys testpy globally
    âœ“ Version shows correctly
    âœ“ Works from any directory
    âœ“ PATH setup instructions clear

  Hints:
    - Copy deploy.sh pattern from feat-py
    - Use sed to inject version: sed "s/__version__ = .*/__version__ = \"$VERSION\"/"
    - Check PATH: echo $PATH | grep -q ~/.local/bin

---

STORY: M6.2 - Init Command [3 points]
  Description: Initialize .spec.toml for new projects

  Tasks:
    - Add 'init' command to CLI
    - Detect project language(s)
    - Generate .spec.toml with defaults
    - Create tests/ directory if missing
    - Show next steps

  Requirements:
    - Auto-detect: Rust (Cargo.toml), Python (pyproject.toml), Node.js (package.json)
    - Generate: .spec.toml with detected languages
    - Defaults: sensible test patterns, exclusions per language
    - Create: tests/ directory structure
    - Output: next steps message

  Success Criteria:
    âœ“ Detects languages correctly
    âœ“ Generates valid .spec.toml
    âœ“ Creates tests/ directory
    âœ“ Shows helpful next steps

  Hints:
    - Template .spec.toml per language
    - Project name: from manifest or directory name
    - Tests root: tests/ (create if missing)
    - Next steps: "Run 'testpy lint' to check test organization"

---

STORY: M6.3 - Migration Guide [2 points]
  Description: Guide for migrating RSB projects to testpy

  Tasks:
    - Write MIGRATION.md
    - Document rsb test.sh â†’ testpy migration
    - Provide migration checklist
    - Show examples for each project type

  Requirements:
    - Cover: projects with existing test.sh
    - Steps: install testpy, init config, validate tests
    - Checklist: pre-migration, migration, post-migration
    - Examples: RSB project, Python project, Node.js project

  Success Criteria:
    âœ“ Clear migration steps
    âœ“ Examples for each language
    âœ“ Checklist is comprehensive
    âœ“ Addresses common issues

  Hints:
    - Structure: Before, During, After sections
    - Before: backup, understand current setup
    - During: install, init, validate
    - After: test, commit, remove old test.sh

---

STORY: M6.4 - Comprehensive README [3 points]
  Description: Update README.md with full documentation

  Tasks:
    - Document all features
    - Add usage examples for all 4 languages
    - Document configuration options
    - Add troubleshooting section

  Requirements:
    - Sections: Installation, Usage, Config, Languages, Commands, Troubleshooting
    - Examples: Rust, Python, Node.js, Shell
    - Config: full .spec.toml reference
    - Commands: all commands with examples

  Success Criteria:
    âœ“ Complete feature coverage
    âœ“ Examples for all languages
    âœ“ Config reference is clear
    âœ“ Troubleshooting covers common issues

  Hints:
    - Follow feat-py README structure
    - Show .spec.toml examples per language
    - Command examples: testpy run sanity, testpy lint, etc.
    - Troubleshooting: "Tests not found", "Invalid config", etc.

---

STORY: M6.5 - Test Coverage for testpy Itself [5 points]
  Description: Test suite for testpy tool

  Tasks:
    - Create tests/ for testpy itself
    - Unit tests for core modules
    - Integration tests for commands
    - Test against sample projects
    - Add CI via GitHub Actions

  Requirements:
    - Unit tests: config, discovery, validation, runner
    - Integration: CLI commands with sample projects
    - Sample projects: fixtures for Rust, Python, Node.js, Shell
    - CI: run tests on push, test against Python 3.8-3.12

  Success Criteria:
    âœ“ >80% code coverage
    âœ“ All commands tested
    âœ“ CI passes on all Python versions
    âœ“ Sample fixtures work correctly

  Hints:
    - Use pytest for testing
    - Fixtures: tests/fixtures/rust-project/, python-project/, etc.
    - Mock: subprocess calls for isolation
    - CI: .github/workflows/test.yml

---

STORY: M6.6 - RSB Project Rollout [5 points]
  Description: Deploy testpy to all 23 RSB projects

  Tasks:
    - Install testpy globally
    - Migrate oodx-rsb first (canonical)
    - Create .spec.toml for each project
    - Validate test structure for each
    - Update HANDOFF.md with progress

  Requirements:
    - Install: testpy to ~/.local/bin/snek/
    - Migrate: prioritize high-priority projects first
    - Validate: run testpy lint on each project
    - Fix: violations before finalizing
    - Document: update HANDOFF.md

  Success Criteria:
    âœ“ testpy deployed globally
    âœ“ All 23 projects have .spec.toml
    âœ“ All projects pass testpy lint
    âœ“ HANDOFF.md updated

  Hints:
    - Start with oodx-rsb (reference)
    - High priority: meteor-meteordb, skull-ignite, vizo-boxy, pronto-prontodb
    - Create script: deploy_all.sh for batch operations
    - Track progress in HANDOFF.md

---

MILESTONE 6 TOTAL: 21 story points

================================================================================
SUMMARY
================================================================================

Total Story Points: 180 points (was 133 + 44 for M4 + 3 for M2.6)
Estimated Timeline: 15 weeks (10-12 points per week)

MILESTONE BREAKDOWN:
  M1: Foundation & Infrastructure           - 16 points (Weeks 1-2) âœ… COMPLETE
  M2: Rust Support                          - 31 points (Weeks 3-4) âœ… COMPLETE (28/31, M2.6 optional)
  M3: Multi-Language Discovery & Validation - 28 points (Weeks 5-6)
  M4: Documentation & Testing UX            - 44 points (Weeks 7-8) ðŸ†•
  M5: Test Execution Engine                 - 21 points (Weeks 9-10)
  M6: Reporting & Polish                    - 19 points (Weeks 11-13)
  M7: Deployment & Documentation            - 21 points (Weeks 14-15)

LANGUAGE SUPPORT TIMELINE:
  - Rust: Milestones 1-2 (Weeks 1-4) - Full support âœ…
  - Python: Milestone 3 + M5.1 (Weeks 5-9) - Full support
  - Node.js: Milestone 3 + M5.2 (Weeks 5-9) - Full support
  - Shell: Milestone 3 + M5.3 (Weeks 5-9) - Full support

RISK AREAS (Higher complexity):
  - M2.3: Rust Test Validation (8 points) - Complex validation logic âœ… DONE
  - M3.x: Multi-language support (28 points) - 4 languages to support
  - M4.7: Ceremony multi-language execution (8 points) - Shell/Rust/Python support
  - M5.4: Parallel Execution (5 points) - Concurrency challenges
  - M5.5: Test Parsing (5 points) - Multiple output formats

DEPENDENCIES:
  - M2 depends on M1 (foundation required) âœ…
  - M3 depends on M2 (Rust as reference implementation)
  - M4 can run parallel to M3 (documentation independent)
  - M5 depends on M3 (discovery before execution)
  - M6 depends on M5 (execution before reporting)
  - M7 depends on M6 (polish before deployment)

================================================================================
NEXT STEPS
================================================================================

1. Review and approve this TASKS.txt
2. Prioritize M1 tasks (foundation)
3. Begin implementation with M1.1 (project structure)
4. Track progress in this file (mark completed stories)
5. Update HANDOFF.md after each milestone

================================================================================
